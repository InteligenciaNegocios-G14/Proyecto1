{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyecto 1 \n",
    "\n",
    "Nicolas Arango - 202220342\n",
    "Mateo Rincon - 202221402\n",
    "Amalia Carbonell - 202122079 \n",
    "\n",
    "\n",
    "\n",
    "1. Entendimiento y preparación de los datos (a nivel de código y análisis).\n",
    "2. Modelado y evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entendimiento y preparación de los datos (a nivel de código y análisis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(os.path.abspath('./nltk_data'))\n",
    "nltk.download('punkt', download_dir='./nltk_data')\n",
    "nltk.download('punktab', download_dir='./nltk_data')\n",
    "nltk.download('stopwords', download_dir='./nltk_data')\n",
    "nltk.download('wordnet', download_dir='./nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_spanish = stopwords.words('spanish')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de Datos\n",
    "#noticias_df = pd.read_csv(\"./data/fake_news_test.csv\", sep=\";\", usecols=['ID','Titulo', \"Descripcion\", \"Fecha\"])\n",
    "datax=pd.read_csv('./data/fake_news_spanish.csv', sep=';', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Es recomendable que todos los pasos de limpieza y preparación se realicen sobre otro archivo.\n",
    "data = datax.copy()\n",
    "# Eliminación de registros duplicados.\n",
    "data=data.drop_duplicates()\n",
    "\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar los tipos de datos en la columna \"Titulo\"\n",
    "tipo_datos = data['Titulo'].apply(type).value_counts()\n",
    "print(tipo_datos)\n",
    "\n",
    "print(data['Titulo'][data['Titulo'].apply(type) == float])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cambiar float a String\n",
    "data['Titulo'] = data['Titulo'].apply(lambda x: str(x) if isinstance(x, float) else x)\n",
    "# cambiar float a String vacio (\"\")\n",
    "data['Titulo'] = data['Titulo'].apply(lambda x: \"\" if x == \"nan\" else x)\n",
    "tipo_datos = data['Titulo'].apply(type).value_counts()\n",
    "print(tipo_datos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar los tipos de datos en la columna \"Descripcion\"\n",
    "tipo_datos = data['Descripcion'].apply(type).value_counts()\n",
    "print(tipo_datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describir los Datos\n",
    "\n",
    "Se hace una descripción más detallada de nuestros datos.Es un paso es muy importante para poder determinar problemas de calidad de datos. Además, nos dará información que será útil en la fase de preparación de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se calcula el porcentaje de los valores nulos por columna\n",
    "print((data.isnull().sum()*100 / data.shape[0]).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar la duplicidad de los datos\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar distribución de noticias falsas\n",
    "data['Label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenguaje natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "stemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    text = re.sub(r'<[^>]*>', '', text)  # Eliminar etiquetas HTML\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Eliminar puntuaciones\n",
    "    text = text.lower().strip()  # Convertir a minúsculas y eliminar espacios extra\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Titulo'] = data['Titulo'].apply(preprocessor)\n",
    "data['Descripcion'] = data['Descripcion'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wpt = nltk.WordPunctTokenizer()\n",
    "# ps = PorterStemmer()\n",
    "# nltk.download('stopwords')\n",
    "# stop_words = nltk.corpus.stopwords.words('spanish')\n",
    "\n",
    "# def normalize_documents(doc):\n",
    "#   doc = re.sub(r'[^a-zA-Z\\s]','',doc, re.I|re.A)\n",
    "#   doc = doc.lower()\n",
    "#   doc = doc.strip()\n",
    "#   tokens = wpt.tokenize(doc)\n",
    "#   filtered_token = [ps.stem(token) for token in tokens if token not in stop_words]\n",
    "#   doc = ' '.join(filtered_token)\n",
    "#   return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize_corpus = np.vectorize(normalize_documents)\n",
    "# data['Titulo'] = data['Titulo'].apply(normalize_corpus)\n",
    "# data['Descripcion'] = data['Descripcion'].apply(normalize_corpus)\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bv = CountVectorizer(ngram_range=(2,3), max_features=50000)#Usa 10,000 palabras mas comunes\n",
    "\n",
    "bv_matrix = bv.fit_transform(data['Titulo'])\n",
    "bv_matrix2 = bv.fit_transform(data['Descripcion'])\n",
    "bv_matrix = bv_matrix.toarray()\n",
    "bv_matrix2 = bv_matrix2.toarray()\n",
    "vocab = bv.get_feature_names_out()\n",
    "pd.DataFrame(bv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(data['Titulo'], data['Label'], test_size=0.3, stratify=data['Label'], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(Y_train).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(Y_test).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(tokenizer=word_tokenize, stop_words=stopwords_spanish, lowercase=True, token_pattern=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow_train = bow.fit_transform(X_train)\n",
    "X_bow_test = bow.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bow.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords_spanish, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
